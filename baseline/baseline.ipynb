{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import distance  \n",
    "import Levenshtein\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from numba import jit\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in d:\\programdata\\miniconda3\\envs\\jupyter\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 22.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install distance\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('data/train.csv',sep='\\t',header=None)\n",
    "train.columns=['q1','q2','label']\n",
    "test=pd.read_csv('data/test.csv',sep='\\t',header=None)\n",
    "test.columns=['q1','q2']\n",
    "test['label']=1\n",
    "sample_submit=pd.read_csv('data/sample_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>有哪些女明星被潜规则啦</td>\n",
       "      <td>哪些女明星被潜规则了</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>怎么支付宝绑定银行卡？</td>\n",
       "      <td>银行卡怎么绑定支付宝</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>请问这部电视剧叫什么名字</td>\n",
       "      <td>请问谁知道这部电视剧叫什么名字</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>泰囧完整版下载</td>\n",
       "      <td>エウテルペ完整版下载</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在沧州市区哪家卖的盐焗鸡好吃？</td>\n",
       "      <td>沧州饭店哪家便宜又好吃又实惠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                q1               q2  label\n",
       "0      有哪些女明星被潜规则啦       哪些女明星被潜规则了      1\n",
       "1      怎么支付宝绑定银行卡？       银行卡怎么绑定支付宝      1\n",
       "2     请问这部电视剧叫什么名字  请问谁知道这部电视剧叫什么名字      1\n",
       "3          泰囧完整版下载       エウテルペ完整版下载      0\n",
       "4  在沧州市区哪家卖的盐焗鸡好吃？   沧州饭店哪家便宜又好吃又实惠      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   q1      5000 non-null   object\n",
      " 1   q2      5000 non-null   object\n",
      " 2   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   q1      5000 non-null   object\n",
      " 1   q2      5000 non-null   object\n",
      " 2   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5784\n",
       "0    0.4216\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "train_size=len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 基础特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本长度特征\n",
    "data['q1_len']=data['q1'].astype(str).map(len)\n",
    "data['q2_len']=data['q2'].astype(str).map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        10.658400\n",
       "std          4.019095\n",
       "min          3.000000\n",
       "25%          8.000000\n",
       "50%         10.000000\n",
       "75%         12.000000\n",
       "max         49.000000\n",
       "Name: q1_len, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q1_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长度差特征：差/比例\n",
    "data['q1q2_len_diff']=data['q1_len']-data['q2_len']\n",
    "data['q1q2_len_diff_abs']=np.abs(data['q1_len']-data['q2_len'])\n",
    "data['q1q2_rate']=data['q1_len']/data['q2_len']\n",
    "data['q2q1_rate']=data['q2_len']/data['q1_len']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 特殊符号特征\n",
    "data['q1_end_special']=data['q1'].str.endswith('？').astype(int)\n",
    "data['q2_end_special']=data['q2'].str.endswith('？').astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 共现字特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comm_q1q2char_nums']=data.apply(lambda  row:len(set(row['q1'])&set(row['q2'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共现字位置\n",
    "def char_match_pos(q1, q2, pos_i):\n",
    "    q1 = list(q1)\n",
    "    q2 = list(q2)\n",
    "\n",
    "    if pos_i < len(q1):\n",
    "        q2_len = min(len(q2), 25)  # q2_len只匹配前25个字\n",
    "        for pos_j in range(q2_len):\n",
    "            if q1[pos_i] == q2[pos_j]:\n",
    "                q_pos = pos_j + 1  # 如果匹配上了 记录匹配的位置\n",
    "                break\n",
    "            elif pos_j == q2_len - 1:\n",
    "                q_pos = 0  # 如果没有匹配上 赋值为0\n",
    "    else:\n",
    "        q_pos = -1  # 如果后续长度不存在 赋值为-1\n",
    "\n",
    "    return q_pos\n",
    "\n",
    "\n",
    "for pos_i in range(8):\n",
    "    data['q1_pos_' + str(pos_i + 1)] = data.apply(\n",
    "        lambda row: char_match_pos(row['q1'], row['q2'], pos_i), axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        4\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "9995     4\n",
       "9996     1\n",
       "9997     0\n",
       "9998    11\n",
       "9999     0\n",
       "Name: q1_pos_1, Length: 10000, dtype: int8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo 这里也可以用结巴分词，改成“词”粒度的\n",
    "data[\"q1_pos_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 距离特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========距离特征 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "距离特征: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"===========距离特征 =============\")\n",
    "sim_func_dict = {\"jaccard\": distance.jaccard,\n",
    "                 \"sorensen\": distance.sorensen,\n",
    "                 \"levenshtein\": distance.levenshtein,\n",
    "                 \"ratio\": Levenshtein.ratio\n",
    "                 }\n",
    "\n",
    "for sim_func in tqdm(sim_func_dict, desc=\"距离特征\"):\n",
    "    data[sim_func] = data.apply(lambda row: sim_func_dict[sim_func](row[\"q1\"],row[\"q2\"]), axis=1)\n",
    "    qt = [[3, 3], [3, 5], [5, 5], [5, 10], [10, 10], [10, 15], [15, 15], [15, 25]]\n",
    "\n",
    "    for qt_len in qt:\n",
    "        if qt_len[0] == 3 and sim_func == \"levenshtein\":\n",
    "            pass\n",
    "        else:\n",
    "            data[sim_func + '_q' + str(qt_len[0]) + '_t' + str(qt_len[1])] = data.apply(\n",
    "                lambda row: sim_func_dict[sim_func](row[\"q1\"][:qt_len[0]],\n",
    "                                                    row[\"q2\"][:qt_len[1]]),\n",
    "                axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 文本向量匹配特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.515 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "data['q1_words_list']=data['q1'].apply(lambda x:[w for w in jieba.cut(x) if w])\n",
    "data['q2_words_list']=data['q2'].apply(lambda x:[w for w in jieba.cut(x) if w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [有, 哪些, 女明星, 被, 潜规则, 啦]\n",
       "1                               [怎么, 支付宝, 绑定, 银行卡, ？]\n",
       "2                            [请问, 这部, 电视剧, 叫, 什么, 名字]\n",
       "3                                     [泰, 囧, 完整版, 下载]\n",
       "4                   [在, 沧州, 市区, 哪家, 卖, 的, 盐焗鸡, 好吃, ？]\n",
       "                            ...                      \n",
       "9995                                    [小额贷款, 怎么, 贷]\n",
       "9996                               [这是, 什么, 乌龟, 阿, ？]\n",
       "9997                                 [如何, 申请, 福利, 企业]\n",
       "9998    [安徽, 三联, 学院, 2015, 新生, 学费, 可以, 开学, 自己, 带去, 吗]\n",
       "9999                 [这, 只能, 说明, 你, 不, 矜持, !, 什么, 意思]\n",
       "Name: q1_words_list, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"q1_words_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['有', '哪些', '女明星', '被', '潜规则', '啦'],\n",
       " ['怎么', '支付宝', '绑定', '银行卡', '？'],\n",
       " ['请问', '这部', '电视剧', '叫', '什么', '名字']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=data['q1_words_list'].values.tolist()+data['q2_words_list'].values.tolist()\n",
    "len(sentences)\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "w2v_model = word2vec.Word2Vec(sentences,\n",
    "                                  vector_size=100, window=10, min_count=1, workers=4,\n",
    "                                  sg=1)\n",
    "w2v_model.save('models/' + 'word2vec.model')\n",
    "w2v_model.wv.save_word2vec_format('models/' + 'word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11027"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 8538.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 8650.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 11096.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 10180.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 11005.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 11042.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 7718.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 10543.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 9067.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 14766.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 14396.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 11602.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine, cityblock, canberra, euclidean, \\\n",
    "    minkowski, braycurtis, correlation, chebyshev, jensenshannon, mahalanobis, \\\n",
    "    seuclidean, sqeuclidean\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# 计算词向量的相似度\n",
    "def get_w2v(query, title, num):\n",
    "    q = np.zeros(100)\n",
    "    count = 0\n",
    "    for w in query:\n",
    "        if w in w2v_model.wv:\n",
    "            q += w2v_model.wv[w]\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        query_vec = q\n",
    "    query_vec = (q / count).tolist()\n",
    "\n",
    "    t = np.zeros(100)\n",
    "    count = 0\n",
    "    for w in title:\n",
    "        if w in w2v_model.wv:\n",
    "            t += w2v_model.wv[w]\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        title_vec = q\n",
    "    title_vec = (t / count).tolist()\n",
    "\n",
    "    if num == 1:\n",
    "        try:\n",
    "            vec_cosine = cosine(query_vec, title_vec)\n",
    "            return vec_cosine\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 2:\n",
    "        try:\n",
    "            vec_canberra = canberra(query_vec, title_vec) / len(query_vec)\n",
    "            return vec_canberra\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 3:\n",
    "        try:\n",
    "            vec_cityblock = cityblock(query_vec, title_vec) / len(query_vec)\n",
    "            return vec_cityblock\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 4:\n",
    "        try:\n",
    "            vec_euclidean = euclidean(query_vec, title_vec)\n",
    "            return vec_euclidean\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 5:\n",
    "        try:\n",
    "            vec_braycurtis = braycurtis(query_vec, title_vec)\n",
    "            return vec_braycurtis\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 6:\n",
    "        try:\n",
    "            vec_minkowski = minkowski(query_vec, title_vec)\n",
    "            return vec_minkowski\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 7:\n",
    "        try:\n",
    "            vec_correlation = correlation(query_vec, title_vec)\n",
    "            return vec_correlation\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "\n",
    "    if num == 8:\n",
    "        try:\n",
    "            vec_chebyshev = chebyshev(query_vec, title_vec)\n",
    "            return vec_chebyshev\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "\n",
    "    if num == 9:\n",
    "        try:\n",
    "            vec_jensenshannon = jensenshannon(query_vec, title_vec)\n",
    "            return vec_jensenshannon\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "\n",
    "    if num == 10:\n",
    "        try:\n",
    "            vec_mahalanobis = mahalanobis(query_vec, title_vec)\n",
    "            return vec_mahalanobis\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "\n",
    "    if num == 11:\n",
    "        try:\n",
    "            vec_seuclidean = seuclidean(query_vec, title_vec)\n",
    "            return vec_seuclidean\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "    if num == 12:\n",
    "        try:\n",
    "            vec_sqeuclidean = sqeuclidean(query_vec, title_vec)\n",
    "            return vec_sqeuclidean\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "# 词向量的相似度特征\n",
    "data['vec_cosine'] = data.progress_apply(lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 1),\n",
    "                                         axis=1)\n",
    "data['vec_canberra'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 2), axis=1)\n",
    "data['vec_cityblock'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 3), axis=1)\n",
    "data['vec_euclidean'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 4), axis=1)\n",
    "data['vec_braycurtis'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 5), axis=1)\n",
    "data['vec_minkowski'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 6), axis=1)\n",
    "data['vec_correlation'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 7), axis=1)\n",
    "\n",
    "data['vec_chebyshev'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 8), axis=1)\n",
    "data['vec_jensenshannon'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 9), axis=1)\n",
    "data['vec_mahalanobis'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 10), axis=1)\n",
    "data['vec_seuclidean'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 11), axis=1)\n",
    "data['vec_sqeuclidean'] = data.progress_apply(\n",
    "    lambda index: get_w2v(index['q1_words_list'], index['q2_words_list'], 12), axis=1)\n",
    "\n",
    "data['vec_cosine'] = data['vec_cosine'].astype('float32')\n",
    "data['vec_canberra'] = data['vec_canberra'].astype('float32')\n",
    "data['vec_cityblock'] = data['vec_cityblock'].astype('float32')\n",
    "data['vec_euclidean'] = data['vec_euclidean'].astype('float32')\n",
    "data['vec_braycurtis'] = data['vec_braycurtis'].astype('float32')\n",
    "data['vec_correlation'] = data['vec_correlation'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.041676\n",
       "1       0.009618\n",
       "2       0.007044\n",
       "3       0.015732\n",
       "4       0.108460\n",
       "          ...   \n",
       "9995    0.069496\n",
       "9996    0.057783\n",
       "9997    0.039493\n",
       "9998    0.048787\n",
       "9999    0.029870\n",
       "Name: vec_cosine, Length: 10000, dtype: float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vec_cosine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 向量特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 10799.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 9465.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def w2v_sent2vec(words):\n",
    "    \"\"\"计算句子的平均word2vec向量, sentences是一个句子, 句向量最后会归一化\"\"\"\n",
    "\n",
    "    M = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            M.append(w2v_model.wv[word])\n",
    "        except KeyError:  # 不在词典里\n",
    "            continue\n",
    "\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return (v / np.sqrt((v ** 2).sum())).astype(np.float32).tolist()\n",
    "\n",
    "\n",
    "fea_names = ['q1_vec_{}'.format(i) for i in range(100)]\n",
    "data[fea_names] = data.progress_apply(lambda row: w2v_sent2vec(row['q1_words_list']), result_type='expand', axis=1)\n",
    "\n",
    "fea_names = ['q2_vec_{}'.format(i) for i in range(100)]\n",
    "data[fea_names] = data.progress_apply(lambda row: w2v_sent2vec(row['q2_words_list']), result_type='expand', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['q1', 'q2', 'label', 'q1_len', 'q2_len', 'q1q2_len_diff',\n",
       "       'q1q2_len_diff_abs', 'q1q2_rate', 'q2q1_rate', 'q1_end_special',\n",
       "       ...\n",
       "       'q2_vec_90', 'q2_vec_91', 'q2_vec_92', 'q2_vec_93', 'q2_vec_94',\n",
       "       'q2_vec_95', 'q2_vec_96', 'q2_vec_97', 'q2_vec_98', 'q2_vec_99'],\n",
       "      dtype='object', length=268)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "['q1_len', 'q2_len', 'q1q2_len_diff', 'q1q2_len_diff_abs', 'q1q2_rate', 'q2q1_rate', 'q1_end_special', 'q2_end_special', 'comm_q1q2char_nums', 'q1_pos_1', 'q1_pos_2', 'q1_pos_3', 'q1_pos_4', 'q1_pos_5', 'q1_pos_6', 'q1_pos_7', 'q1_pos_8', 'jaccard', 'jaccard_q3_t3', 'jaccard_q3_t5', 'jaccard_q5_t5', 'jaccard_q5_t10', 'jaccard_q10_t10', 'jaccard_q10_t15', 'jaccard_q15_t15', 'jaccard_q15_t25', 'sorensen', 'sorensen_q3_t3', 'sorensen_q3_t5', 'sorensen_q5_t5', 'sorensen_q5_t10', 'sorensen_q10_t10', 'sorensen_q10_t15', 'sorensen_q15_t15', 'sorensen_q15_t25', 'levenshtein', 'levenshtein_q5_t5', 'levenshtein_q5_t10', 'levenshtein_q10_t10', 'levenshtein_q10_t15', 'levenshtein_q15_t15', 'levenshtein_q15_t25', 'ratio', 'ratio_q3_t3', 'ratio_q3_t5', 'ratio_q5_t5', 'ratio_q5_t10', 'ratio_q10_t10', 'ratio_q10_t15', 'ratio_q15_t15', 'ratio_q15_t25', 'vec_cosine', 'vec_canberra', 'vec_cityblock', 'vec_euclidean', 'vec_braycurtis', 'vec_minkowski', 'vec_correlation', 'vec_chebyshev', 'vec_jensenshannon', 'vec_mahalanobis', 'vec_seuclidean', 'vec_sqeuclidean', 'q1_vec_0', 'q1_vec_1', 'q1_vec_2', 'q1_vec_3', 'q1_vec_4', 'q1_vec_5', 'q1_vec_6', 'q1_vec_7', 'q1_vec_8', 'q1_vec_9', 'q1_vec_10', 'q1_vec_11', 'q1_vec_12', 'q1_vec_13', 'q1_vec_14', 'q1_vec_15', 'q1_vec_16', 'q1_vec_17', 'q1_vec_18', 'q1_vec_19', 'q1_vec_20', 'q1_vec_21', 'q1_vec_22', 'q1_vec_23', 'q1_vec_24', 'q1_vec_25', 'q1_vec_26', 'q1_vec_27', 'q1_vec_28', 'q1_vec_29', 'q1_vec_30', 'q1_vec_31', 'q1_vec_32', 'q1_vec_33', 'q1_vec_34', 'q1_vec_35', 'q1_vec_36', 'q1_vec_37', 'q1_vec_38', 'q1_vec_39', 'q1_vec_40', 'q1_vec_41', 'q1_vec_42', 'q1_vec_43', 'q1_vec_44', 'q1_vec_45', 'q1_vec_46', 'q1_vec_47', 'q1_vec_48', 'q1_vec_49', 'q1_vec_50', 'q1_vec_51', 'q1_vec_52', 'q1_vec_53', 'q1_vec_54', 'q1_vec_55', 'q1_vec_56', 'q1_vec_57', 'q1_vec_58', 'q1_vec_59', 'q1_vec_60', 'q1_vec_61', 'q1_vec_62', 'q1_vec_63', 'q1_vec_64', 'q1_vec_65', 'q1_vec_66', 'q1_vec_67', 'q1_vec_68', 'q1_vec_69', 'q1_vec_70', 'q1_vec_71', 'q1_vec_72', 'q1_vec_73', 'q1_vec_74', 'q1_vec_75', 'q1_vec_76', 'q1_vec_77', 'q1_vec_78', 'q1_vec_79', 'q1_vec_80', 'q1_vec_81', 'q1_vec_82', 'q1_vec_83', 'q1_vec_84', 'q1_vec_85', 'q1_vec_86', 'q1_vec_87', 'q1_vec_88', 'q1_vec_89', 'q1_vec_90', 'q1_vec_91', 'q1_vec_92', 'q1_vec_93', 'q1_vec_94', 'q1_vec_95', 'q1_vec_96', 'q1_vec_97', 'q1_vec_98', 'q1_vec_99', 'q2_vec_0', 'q2_vec_1', 'q2_vec_2', 'q2_vec_3', 'q2_vec_4', 'q2_vec_5', 'q2_vec_6', 'q2_vec_7', 'q2_vec_8', 'q2_vec_9', 'q2_vec_10', 'q2_vec_11', 'q2_vec_12', 'q2_vec_13', 'q2_vec_14', 'q2_vec_15', 'q2_vec_16', 'q2_vec_17', 'q2_vec_18', 'q2_vec_19', 'q2_vec_20', 'q2_vec_21', 'q2_vec_22', 'q2_vec_23', 'q2_vec_24', 'q2_vec_25', 'q2_vec_26', 'q2_vec_27', 'q2_vec_28', 'q2_vec_29', 'q2_vec_30', 'q2_vec_31', 'q2_vec_32', 'q2_vec_33', 'q2_vec_34', 'q2_vec_35', 'q2_vec_36', 'q2_vec_37', 'q2_vec_38', 'q2_vec_39', 'q2_vec_40', 'q2_vec_41', 'q2_vec_42', 'q2_vec_43', 'q2_vec_44', 'q2_vec_45', 'q2_vec_46', 'q2_vec_47', 'q2_vec_48', 'q2_vec_49', 'q2_vec_50', 'q2_vec_51', 'q2_vec_52', 'q2_vec_53', 'q2_vec_54', 'q2_vec_55', 'q2_vec_56', 'q2_vec_57', 'q2_vec_58', 'q2_vec_59', 'q2_vec_60', 'q2_vec_61', 'q2_vec_62', 'q2_vec_63', 'q2_vec_64', 'q2_vec_65', 'q2_vec_66', 'q2_vec_67', 'q2_vec_68', 'q2_vec_69', 'q2_vec_70', 'q2_vec_71', 'q2_vec_72', 'q2_vec_73', 'q2_vec_74', 'q2_vec_75', 'q2_vec_76', 'q2_vec_77', 'q2_vec_78', 'q2_vec_79', 'q2_vec_80', 'q2_vec_81', 'q2_vec_82', 'q2_vec_83', 'q2_vec_84', 'q2_vec_85', 'q2_vec_86', 'q2_vec_87', 'q2_vec_88', 'q2_vec_89', 'q2_vec_90', 'q2_vec_91', 'q2_vec_92', 'q2_vec_93', 'q2_vec_94', 'q2_vec_95', 'q2_vec_96', 'q2_vec_97', 'q2_vec_98', 'q2_vec_99']\n"
     ]
    }
   ],
   "source": [
    "no_feas=['q1','q2','label','q1_words_list','q2_words_list']\n",
    "features=[col for col in data.columns if col not in no_feas]\n",
    "\n",
    "train,test=data[:train_size],data[train_size:]\n",
    "print(len(features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features] # 训练集输入\n",
    "y = train['label'] # 训练集标签\n",
    "X_test = test[features] # 测试集输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True,random_state=1314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=450, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=450\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.2, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.2\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.350516\tvalid_1's binary_logloss: 0.405855\n",
      "[100]\ttraining's binary_logloss: 0.309711\tvalid_1's binary_logloss: 0.389669\n",
      "[150]\ttraining's binary_logloss: 0.282233\tvalid_1's binary_logloss: 0.382597\n",
      "[200]\ttraining's binary_logloss: 0.26038\tvalid_1's binary_logloss: 0.38043\n",
      "[250]\ttraining's binary_logloss: 0.240912\tvalid_1's binary_logloss: 0.37968\n",
      "[300]\ttraining's binary_logloss: 0.223483\tvalid_1's binary_logloss: 0.378818\n",
      "[350]\ttraining's binary_logloss: 0.208562\tvalid_1's binary_logloss: 0.381546\n",
      "[400]\ttraining's binary_logloss: 0.194883\tvalid_1's binary_logloss: 0.382515\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's binary_logloss: 0.246771\tvalid_1's binary_logloss: 0.378248\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=450, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=450\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.2, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.2\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.352481\tvalid_1's binary_logloss: 0.395883\n",
      "[100]\ttraining's binary_logloss: 0.309016\tvalid_1's binary_logloss: 0.38409\n",
      "[150]\ttraining's binary_logloss: 0.28006\tvalid_1's binary_logloss: 0.378967\n",
      "[200]\ttraining's binary_logloss: 0.257961\tvalid_1's binary_logloss: 0.375689\n",
      "[250]\ttraining's binary_logloss: 0.238856\tvalid_1's binary_logloss: 0.374809\n",
      "[300]\ttraining's binary_logloss: 0.222109\tvalid_1's binary_logloss: 0.376114\n",
      "[350]\ttraining's binary_logloss: 0.206925\tvalid_1's binary_logloss: 0.374966\n",
      "[400]\ttraining's binary_logloss: 0.193683\tvalid_1's binary_logloss: 0.376411\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's binary_logloss: 0.252454\tvalid_1's binary_logloss: 0.373634\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=450, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=450\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.2, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.2\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.356698\tvalid_1's binary_logloss: 0.372419\n",
      "[100]\ttraining's binary_logloss: 0.313481\tvalid_1's binary_logloss: 0.357385\n",
      "[150]\ttraining's binary_logloss: 0.285644\tvalid_1's binary_logloss: 0.355185\n",
      "[200]\ttraining's binary_logloss: 0.263234\tvalid_1's binary_logloss: 0.352512\n",
      "[250]\ttraining's binary_logloss: 0.244405\tvalid_1's binary_logloss: 0.353096\n",
      "[300]\ttraining's binary_logloss: 0.227838\tvalid_1's binary_logloss: 0.352343\n",
      "[350]\ttraining's binary_logloss: 0.213428\tvalid_1's binary_logloss: 0.350859\n",
      "[400]\ttraining's binary_logloss: 0.200127\tvalid_1's binary_logloss: 0.351173\n",
      "[450]\ttraining's binary_logloss: 0.188107\tvalid_1's binary_logloss: 0.351571\n",
      "[500]\ttraining's binary_logloss: 0.177361\tvalid_1's binary_logloss: 0.352497\n",
      "Early stopping, best iteration is:\n",
      "[329]\ttraining's binary_logloss: 0.21925\tvalid_1's binary_logloss: 0.349632\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=450, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=450\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.2, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.2\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.363254\tvalid_1's binary_logloss: 0.352903\n",
      "[100]\ttraining's binary_logloss: 0.322087\tvalid_1's binary_logloss: 0.335863\n",
      "[150]\ttraining's binary_logloss: 0.295166\tvalid_1's binary_logloss: 0.326221\n",
      "[200]\ttraining's binary_logloss: 0.273144\tvalid_1's binary_logloss: 0.323382\n",
      "[250]\ttraining's binary_logloss: 0.25339\tvalid_1's binary_logloss: 0.321069\n",
      "[300]\ttraining's binary_logloss: 0.236884\tvalid_1's binary_logloss: 0.319676\n",
      "[350]\ttraining's binary_logloss: 0.221445\tvalid_1's binary_logloss: 0.318225\n",
      "[400]\ttraining's binary_logloss: 0.207951\tvalid_1's binary_logloss: 0.317179\n",
      "[450]\ttraining's binary_logloss: 0.195469\tvalid_1's binary_logloss: 0.318549\n",
      "[500]\ttraining's binary_logloss: 0.184338\tvalid_1's binary_logloss: 0.316945\n",
      "[550]\ttraining's binary_logloss: 0.173771\tvalid_1's binary_logloss: 0.317803\n",
      "[600]\ttraining's binary_logloss: 0.164199\tvalid_1's binary_logloss: 0.316342\n",
      "[650]\ttraining's binary_logloss: 0.155377\tvalid_1's binary_logloss: 0.316236\n",
      "[700]\ttraining's binary_logloss: 0.147396\tvalid_1's binary_logloss: 0.316818\n",
      "[750]\ttraining's binary_logloss: 0.140216\tvalid_1's binary_logloss: 0.316917\n",
      "[800]\ttraining's binary_logloss: 0.133344\tvalid_1's binary_logloss: 0.316716\n",
      "Early stopping, best iteration is:\n",
      "[611]\ttraining's binary_logloss: 0.162196\tvalid_1's binary_logloss: 0.315673\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=450, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=450\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.2, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.2\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.95, subsample=1.0 will be ignored. Current value: bagging_fraction=0.95\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.001, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's binary_logloss: 0.349151\tvalid_1's binary_logloss: 0.404616\n",
      "[100]\ttraining's binary_logloss: 0.306807\tvalid_1's binary_logloss: 0.390282\n",
      "[150]\ttraining's binary_logloss: 0.278249\tvalid_1's binary_logloss: 0.388941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's binary_logloss: 0.255646\tvalid_1's binary_logloss: 0.388844\n",
      "[250]\ttraining's binary_logloss: 0.237118\tvalid_1's binary_logloss: 0.388846\n",
      "[300]\ttraining's binary_logloss: 0.220535\tvalid_1's binary_logloss: 0.390432\n",
      "[350]\ttraining's binary_logloss: 0.205238\tvalid_1's binary_logloss: 0.391897\n",
      "[400]\ttraining's binary_logloss: 0.19206\tvalid_1's binary_logloss: 0.394902\n",
      "Early stopping, best iteration is:\n",
      "[245]\ttraining's binary_logloss: 0.238918\tvalid_1's binary_logloss: 0.387534\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 5,\n",
    "    'max_depth': 6,\n",
    "    'min_data_in_leaf': 450,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 1,  \n",
    "    'lambda_l2': 0.001,  # 越小l2正则程度越高\n",
    "    'min_gain_to_split': 0.2,\n",
    "}\n",
    " \n",
    "oof = np.zeros(len(X))\n",
    "prediction = np.zeros(len(X_test))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "    X_train, X_valid = X[features].iloc[train_index], X[features].iloc[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators=50000, n_jobs=-1)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_metric='binary_logloss',\n",
    "              verbose=50, early_stopping_rounds=200)\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "    oof[valid_index] = y_pred_valid.reshape(-1, )\n",
    "    prediction += y_pred\n",
    "prediction /= n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8388"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = (oof > 0.5)\n",
    "# score=accuracy_score(np.round(abs(oof)) ,train['label'].values)\n",
    "score=accuracy_score(y_pred ,train['label'].values)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = (prediction > 0.5).astype(int)\n",
    "sample_submit['label']=sub_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit[['label']].to_csv('lgb.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2906\n",
       "0    2094\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
